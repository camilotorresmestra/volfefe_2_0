{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MCv_Xo_MYqCM"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import snscrape.modules.twitter as twtr\n",
        "from pymongo import MongoClient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters\n",
        "TWITTER_HANDLE = 'petrogustavo' # Gustavo Petro's twitter handle\n",
        "BEGIN_DATE = '2022-08-07' # Initial day of the Gustavo Petro's presidency\n",
        "END_DATE = '2023-06-24' # Current day"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 1. Scraping Tweets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this section we'll make use of the sns package, that allows us to extract tweets from Twitter without the need of a Twitter API. The package is available at (INSERT LINK). \n",
        "\n",
        "We start by using the TwitterProfileScraper function to obtain all tweets related to the user @petrogustavo. The function returns a list of dictionaries, where each dictionary corresponds to a tweet. The function takes as input the username, the number of tweets to be extracted, and the language of the tweets. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scraping tweets:   3%|▎         | 343/10000 [00:16<07:11, 22.38tweets/s]Unavailable user in card on tweet 1666268373640966145\n",
            "User 897095448242130944 not found in user refs in card on tweet 1666268373640966145\n",
            "Unavailable user in card on tweet 1666272920006451204\n",
            "User 897095448242130944 not found in user refs in card on tweet 1666272920006451204\n",
            "Scraping tweets:   8%|▊         | 758/10000 [00:34<06:36, 23.32tweets/s]Unavailable user in card on tweet 1658563722133098513\n",
            "User 897095448242130944 not found in user refs in card on tweet 1658563722133098513\n",
            "Unavailable user in card on tweet 1658585152514301953\n",
            "User 897095448242130944 not found in user refs in card on tweet 1658585152514301953\n",
            "Scraping tweets:  14%|█▍        | 1425/10000 [01:03<05:53, 24.24tweets/s]Unavailable user in card on tweet 1646098122676871168\n",
            "User 897095448242130944 not found in user refs in card on tweet 1646098122676871168\n",
            "Unavailable user in card on tweet 1646121710456918016\n",
            "User 897095448242130944 not found in user refs in card on tweet 1646121710456918016\n",
            "Scraping tweets:  32%|███▏      | 3228/10000 [02:17<04:15, 26.53tweets/s]Stopping after 20 empty pages\n",
            "Scraping tweets:  32%|███▏      | 3244/10000 [02:22<04:57, 22.69tweets/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tweets saved:  3243\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "import sys\n",
        "import json\n",
        "sys.stdout.encoding = 'utf-8'\n",
        "scraper = twtr.TwitterSearchScraper(f'from:{TWITTER_HANDLE}'\n",
        "                                    #since:{BEGIN_DATE} until:{END_DATE}'\n",
        "                                    )\n",
        "NUMBER_OF_TWEETS = int(10_000) # Number of tweets to be scraped\n",
        "# Get the tweets\n",
        "profile_scraper = twtr.TwitterProfileScraper(TWITTER_HANDLE)\n",
        "tweets = profile_scraper.get_items()\n",
        "# print all properties of the tweet\n",
        "# save the tweets in a pandas dataframe\n",
        "with open('data/tweets.json', 'w+', encoding='utf-8') as f:\n",
        "    f.write('{ \"tweet\":[')\n",
        "    # iterate over the tweets using tqdm to show a progress bar NUMBER_OF_TWEETS times\n",
        "    for i,tweet in tqdm(enumerate(tweets), total = NUMBER_OF_TWEETS, desc='Scraping tweets', unit='tweets'):\n",
        "        # create a dictionary with the tweet information\n",
        "        tweet_dict = {'id': tweet.id,\n",
        "                      'date': tweet.date.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                        'content': tweet.retweetedTweet.rawContent if tweet.retweetedTweet else tweet.rawContent,\n",
        "                        'url': tweet.url,\n",
        "                        'replyCount': tweet.replyCount if tweet.replyCount else 0,\n",
        "                        'retweetCount': tweet.retweetCount if tweet.retweetCount else 0,\n",
        "                        'likeCount': tweet.likeCount if tweet.likeCount else 0,\n",
        "                        'quoteCount': tweet.quoteCount if tweet.quoteCount else 0,\n",
        "                        'isRetweet': True if tweet.retweetedTweet else False,\n",
        "                        'isReply': True if tweet.inReplyToTweetId else False,\n",
        "                        'isQuote': True if tweet.quotedTweet else False,\n",
        "                        'retweetedFromUser' : tweet.retweetedTweet.user.username if tweet.retweetedTweet else None,\n",
        "                        'mentionedUsers': [user.username for user in tweet.mentionedUsers] if tweet.mentionedUsers else None,\n",
        "                        #'hashtags': [hashtag.text for hashtag in tweet.hashtags],\n",
        "                        'hasMedia': True if tweet.media else False,\n",
        "        }\n",
        "        # save the tweet in the json file\n",
        "        json.dump(tweet_dict, f, ensure_ascii=False);\n",
        "        # add a comma to separate the tweets\n",
        "        f.write(',');\n",
        "        # break condition\n",
        "        if tweet.date.strftime('%Y-%m-%d') < BEGIN_DATE:\n",
        "            break\n",
        "    print('Number of tweets saved: ', i)\n",
        "    # remove the last comma\n",
        "    f.seek(f.tell() - 1, 0)\n",
        "    f.write(']}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 2. Scraping USD/COP exchange rate historical data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we need to load the read the data from the API and load it onto the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success!\n"
          ]
        }
      ],
      "source": [
        "# Setup forex API\n",
        "import requests\n",
        "import json\n",
        "# load API key from file\n",
        "with open('creds/forex_api_token.txt', 'r') as f:\n",
        "    api_token = f.read()\n",
        "# Set uo API request\n",
        "base_url = \"https://api.markets.sh/api/v1/symbols/{symbols}/{method}?\".format(symbols='USDCOP', method='quotes')\n",
        "params = {'api_token': api_token, 'from': BEGIN_DATE, 'to': END_DATE}\n",
        "# make request\n",
        "response = requests.get(base_url, params=params)\n",
        "# save response to file if successful\n",
        "if response.status_code == 200:\n",
        "    print('Success!')\n",
        "    # create file if it doesn't exist\n",
        "    with open('data/forex_data.json', 'w+') as f:\n",
        "        json.dump(response.json(), f)\n",
        "else:\n",
        "    print('Error! Status code: {}'.format(response.status_code))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 3. Database configuration"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once we've pulled all the tweets we need, we'll store them in a database. We will use MongoDB, a NoSQL database that stores data in JSON-like documents, taking advantage of the fact that the tweets are already in JSON format. And that NoSQL databases are more flexible than SQL databases, allowing us to store data without a predefined schema, which is useful since many tweets don't have the same fields (for example, retweets or replies don't have the same fields as regular tweets)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's begin with the database configuration, if you require the credentials to access the database, please contact me at camilotorresmestra@gmail.com "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to MongoDB\n",
        "import json\n",
        "with open('creds/mongo.json') as f:\n",
        "    data = json.load(f)\n",
        "    __db__ = data['db']\n",
        "    __usr__ = data['user']\n",
        "    __pass__ = data['password']\n",
        "\n",
        "connection_str = \"mongodb+srv://{usr}:{passwrd}@{db}.xwk6g0f.mongodb.net/?retryWrites=true&w=majority\".format(\n",
        "#connection_str = \"mongodb+srv://{usr}:{passwrd}@{db}.durge5s.mongodb.net/?retryWrites=true&w=majority\".format(\n",
        "    \n",
        "    usr=__usr__, passwrd=__pass__, db=__db__\n",
        ");\n",
        "client = MongoClient(connection_str)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Create a collection called 'tweets' in the database 'volfefe' in MongoDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create database and collection if they don't exist already\n",
        "db = client['mlds3']\n",
        "if \"tweets\" in db.list_collection_names():\n",
        "    db.drop_collection(\"tweets\")\n",
        "collection = db['tweets']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, insert the scraped tweets into the database using the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<pymongo.results.InsertManyResult at 0x1a8450adff0>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# only execute this cell when the tweets collection is full\n",
        "data = []\n",
        "with open('data/tweets.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)['tweet']\n",
        "collection.insert_many(data)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a5MByQlFZBPh"
      },
      "source": [
        "Pull a tweet from the collection to verify the structure of the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'_id': ObjectId('64977a0e21cc58945cb8f73d'),\n",
              " 'id': 1671252387518988288,\n",
              " 'date': '2023-06-20 20:23:14',\n",
              " 'content': 'El hundimienho de la reforma laboral es muy grave. Demuestra que la voluntad de paz y de pacto social no existe en el poder económico. Dueños del capital y de los medios lograron cooptar el Congreso en contra de la dignidad del pueblo trabajador.\\n\\nCreen que las ganancias salen de la esclavitud, las largas jornadas y la completa inestabilidad laboral.\\n\\nEl gobierno del cambio no abandonará los intereses de la trabajadora y el trabajador.',\n",
              " 'url': 'https://twitter.com/petrogustavo/status/1671252387518988288',\n",
              " 'replyCount': 12195,\n",
              " 'retweetCount': 11802,\n",
              " 'likeCount': 34732,\n",
              " 'quoteCount': 1660,\n",
              " 'isRetweet': False,\n",
              " 'isReply': False,\n",
              " 'isQuote': False,\n",
              " 'retweetedFromUser': None,\n",
              " 'mentionedUsers': None,\n",
              " 'hasMedia': False}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "collection.find_one()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's create a collection called 'usd_cop' in the same database in MongoDB to store the USD/COP exchange rate data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<pymongo.results.InsertManyResult at 0x1a842e33a90>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load data from file\n",
        "data = []\n",
        "with open('data/forex_data.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)['bars']\n",
        "\n",
        "# Create database and collection if they don't exist already\n",
        "db = client['mlds3']\n",
        "if \"forex\" in db.list_collection_names():\n",
        "    db.drop_collection(\"forex\")\n",
        "collection = db['forex']\n",
        "\n",
        "# insert data into collection\n",
        "collection.insert_many(data)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pull a document from the collection to verify the structure of the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'_id': ObjectId('64977aa121cc58945cb903e9'),\n",
              " 'close': 4343.35,\n",
              " 'date': '2022-08-07',\n",
              " 'volume': 2}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "collection.find_one()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 4 . Data cleaning and preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "e234a3f0d45b6f3d988fa6a9d6e5ae5cff8876d1ccfe61cb1d38e84be021dfe2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
